# otak-conference Project Technical Documentation and Rules (v0.8.0)

Updated to reflect the current implementation with WebGL-based generative art background, enhanced audio processing, and comprehensive testing infrastructure.

## Key Updates in v0.8.0:
- **WebGL Generative Art**: High-performance particle system background with 5000+ particles
- **Enhanced Audio Processing**: AudioWorklet-based PCM processing and advanced audio capture
- **Conference Interaction**: Dynamic particle behaviors based on conference state and Gemini speaking
- **Improved Testing**: Comprehensive test coverage including WebGL component testing
- **Performance Optimization**: GPU-accelerated rendering and optimized audio pipeline

# otak-conference Project Rules and Understanding

## Project Overview
otak-conference is a real-time translation conference application that enables multilingual communication using WebRTC and Gemini API.

## Architecture
- **Frontend**: React (TypeScript) with Tailwind CSS - **MODULAR ARCHITECTURE**
- **Backend**: Cloudflare Workers with Durable Objects - **MODULAR ARCHITECTURE**
- **Real-time Communication**: WebRTC for audio/video, WebSocket for signaling
- **Translation**: Google Gemini API for speech-to-text and translation
- **Live Audio Translation**: Gemini 2.5 Flash Native Audio Dialog for real-time audio translation
- **Deployment**: GitHub Pages for static frontend, Cloudflare Workers for backend

### Frontend Modular Architecture
- **types.ts**: Interface and type definitions (Participant, Translation, ChatMessage, AudioTranslation, VoiceSettings, ApiUsageStats, TokenUsage, LocalPlaybackSettings)
- **hooks.ts**: Custom hook useConferenceApp with comprehensive WebRTC/WebSocket/Gemini integration logic
- **components.tsx**: UI components and JSX structure (ConferenceApp component)
- **main.tsx**: Application entry point and DOM mounting
- **gemini-live-audio.ts**: Gemini Live Audio streaming, real-time translation, and audio processing (GeminiLiveAudioStream class)
- **gemini-utils.ts**: Utility functions for Gemini audio processing (createBlob, decode, decodeAudioData, float32ToBase64PCM)
- **debug-utils.ts**: Debug utility functions with conditional logging based on URL debug parameter
- **translation-prompts.ts**: Comprehensive multilingual system prompts and language management
- **generative-art-background-webgl.tsx**: WebGL-based generative art background with particle system and conference interactions

### Backend Modular Architecture
- **worker.js**: Lightweight WebSocket signaling server (no HTML content)
- **room-handler.js**: Durable Object class (RoomDurableObject) for WebSocket room management with participant limits
- **Benefits**: Clear separation of concerns, GitHub Pages serves frontend, Cloudflare handles signaling
- **Deployment**: Frontend on GitHub Pages, Backend signaling on Cloudflare Workers
- **CI/CD**: Automated deployment via GitHub Actions to both platforms
- **Domain**: otak-conference-worker.systemexe-research-and-development.workers.dev

## Key Classes and Methods Understanding

### 1. Core Hook Architecture (hooks.ts)
**Class**: `useConferenceApp` (Custom React Hook)
- **Primary Function**: Manages entire conference application state and functionality
- **Key State Management**:
  - Connection states: `isConnected`, `isInConference`, `isMuted`, `isScreenSharing`, `isCameraOn`
  - User settings: `apiKey`, `username`, `myLanguage`, `selectedMicrophone`, `selectedSpeaker`
  - Communication: `participants`, `translations`, `chatMessages`, `audioTranslations`
  - Audio features: `isAudioTranslationEnabled`, `isLocalPlaybackEnabled`, `sendRawAudio`
  - UI states: `showSettings`, `showChat`, `showReactions`, `showAudioSettings`

**Key Methods**:
- `startConference()`: Initializes WebRTC connections, audio streams, and Gemini Live Audio
- `endConference()`: Cleanly shuts down all connections and audio processing
- `connectToSignaling()`: Establishes WebSocket connection with message handling
- `createPeerConnection()`: Creates WebRTC peer connections with track management
- `updateGeminiTargetLanguage()`: Dynamically updates translation target based on participants
- `sendTranslatedAudioToParticipants()`: Distributes translated audio via WebSocket
- `setupAudioLevelDetection()`: Implements real-time speaking detection
- `toggleMute()`, `toggleScreenShare()`, `toggleCamera()`: Media control methods
- `sendReaction()`, `sendChatMessage()`: Interactive feature methods
- `changeMicrophone()`, `changeSpeaker()`: Audio device management

### 2. Gemini Live Audio Integration (gemini-live-audio.ts)
**Class**: `GeminiLiveAudioStream`
- **Primary Function**: Real-time audio translation using Gemini 2.5 Flash Native Audio Dialog
- **Architecture**: Separate audio contexts for input (16kHz) and output (24kHz) following Google's sample

**Key Properties**:
- `session`: Gemini Live API session instance
- `inputAudioContext`/`outputAudioContext`: Separate audio contexts for processing
- `scriptProcessor`: Audio worklet for real-time capture
- `audioBuffer`: Buffered audio chunks for rate limiting
- `localPlaybackEnabled`: Controls whether to play Gemini responses locally

**Key Methods**:
- `start(mediaStream)`: Initializes Gemini session and audio processing pipeline
- `stop()`: Cleanly shuts down session and audio contexts
- `initializeSession()`: Creates Gemini session with proper configuration
- `setupAudioProcessing()`: Establishes audio capture and processing chain
- `sendBufferedAudio()`: Sends accumulated audio chunks to Gemini
- `handleServerMessage()`: Processes Gemini responses (audio/text)
- `playAudioResponse()`: Plays translated audio with local playback control
- `updateTargetLanguage()`: Recreates session for new translation targets
- `setLocalPlaybackEnabled()`: Controls local audio playback

**Audio Processing Pipeline**:
1. Capture microphone audio at 16kHz
2. Buffer audio chunks (1.5 second intervals)
3. Convert to base64 PCM and send to Gemini
4. Receive translated audio at 24kHz
5. Play locally (if enabled) and distribute to participants

### 3. Room Management (room-handler.js)
**Class**: `RoomDurableObject`
- **Primary Function**: WebSocket room management with participant limits and message broadcasting
- **State Management**: Maintains Map of active sessions with participant metadata

**Key Methods**:
- `fetch(request)`: Handles WebSocket upgrade requests
- `handleSession(webSocket)`: Manages individual WebSocket connections
- `broadcast(message, excludeId)`: Distributes messages to all participants
- **Message Types Handled**: 'join', 'offer', 'answer', 'ice-candidate', 'hand-raise', 'reaction', 'chat', 'message-read', 'speaking-status', 'translated-audio'

**Room Features**:
- Maximum 2 participants per room
- Real-time participant list synchronization
- WebRTC signaling relay
- Interactive feature broadcasting (chat, reactions, hand raise)
- Translated audio distribution

### 4. UI Component Architecture (components.tsx)
**Class**: `ConferenceApp` (React Functional Component)
- **Primary Function**: Renders complete conference interface with all controls and modals
- **Props Interface**: Comprehensive interface with 80+ props for complete state management

**Key UI Sections**:
- **Header**: Project branding, API usage display, settings toggle
- **Settings Panel**: User configuration (username, API key, language)
- **Screen Share Preview**: Local and remote screen share display
- **Main Content**: 3-column layout (participants, translations, screen share)
- **Footer Controls**: Media controls, interactive features
- **Modals**: Camera settings, audio settings, error display, chat panel

**Interactive Features**:
- **Responsive Design**: Mobile/desktop layouts with conditional rendering
- **Real-time Updates**: Participant status, speaking indicators, reactions
- **Device Management**: Audio device selection with live switching
- **Chat System**: Real-time messaging with read receipts
- **Error Handling**: Comprehensive error modals with user-friendly messages

### 5. WebGL Generative Art System (generative-art-background-webgl.tsx)
**Class**: `GenerativeArtBackgroundWebGL` (React Functional Component)
- **Primary Function**: Creates high-performance particle-based generative art background using WebGL
- **Architecture**: GPU-accelerated rendering with custom shaders and Perlin noise-based flow fields

**Key Properties**:
- `particleCount`: 5000 particles for complex visual effects
- `PerlinNoise`: Custom noise generator for organic flow field patterns
- `WebGL Context`: Hardware-accelerated rendering with custom vertex/fragment shaders
- `Conference Integration`: Dynamic particle behavior based on conference state

**Key Features**:
- **Interactive Particles**: Mouse/touch influence on particle movement
- **Conference Awareness**: Special Gemini avatar formation when in conference
- **Real-time Response**: Particle behavior changes when Gemini is speaking
- **Performance**: GPU rendering with 60fps target and optimized buffer management
- **Visual Effects**: Multi-colored particle trails with fade-in/fade-out animations
- **Responsive Design**: Automatic canvas resizing and mobile/desktop optimization

**Audio Integration**:
- `isInConference`: Triggers orbital particle patterns around Gemini avatar
- `onGeminiSpeaking`: Increases particle movement speed and attraction force
- Dynamic particle formations with pulsing effects synchronized to audio translation

### 6. Type System (types.ts)
**Core Interfaces**:
- `Participant`: User metadata with real-time status (speaking, reactions, hand raise)
- `ChatMessage`: Messaging with read receipt tracking
- `Translation`: Audio translation records with metadata
- `AudioTranslation`: Extended translation with audio URL
- `VoiceSettings`: Gemini voice configuration
- `ApiUsageStats`: Token usage tracking for cost management
- `LocalPlaybackSettings`: Audio playback control

### 7. Translation System (translation-prompts.ts)
**Class**: `LanguagePromptManager` (Singleton)
- **Primary Function**: Manages multilingual system prompts and language detection
- **Supported Languages**: 25+ languages with native names and fallback support

**Key Methods**:
- `getSystemPrompt(languageCode)`: Returns language-specific translation prompts
- `getLanguageConfig(languageCode)`: Complete language configuration with fallbacks
- `createMultiParticipantPrompt()`: Dynamic prompts for multi-language scenarios
- `isLanguageSupported()`: Language validation
- `getNativeName()`: Native language names for UI display

**Language Features**:
- Regional variant support (en-US, en-GB, es-ES, es-MX, etc.)
- Strict translation-only prompts (prevents AI responses to questions)
- Cultural adaptation for different regions
- Fallback language mapping

### 8. Debug System (debug-utils.ts)
**Functions**: Conditional logging system based on URL parameter `debug=true`
- `debugLog()`: Development logging (only in debug mode)
- `debugWarn()`: Warning messages (only in debug mode)
- `debugError()`: Error logging (always shows main error, debug details in debug mode)
- `isDebugEnabled()`: Checks URL for debug parameter

### 9. Utility Functions (gemini-utils.ts)
**Audio Processing Utilities**:
- `createBlob(pcmData)`: Converts Float32Array to Blob for Gemini
- `decode(base64)`: Decodes base64 to ArrayBuffer
- `decodeAudioData()`: Converts audio data to AudioBuffer with fallbacks
- `float32ToBase64PCM()`: Converts audio data for Gemini transmission

**Global Audio Functions**:
- `playAudioData()`: Plays translated audio with device selection
- `initializeStreamingAudio()`: Sets up MediaSource for audio streaming
- `initializePCMWorklet()`: AudioWorklet-based PCM playbook

### 10. Audio Processing System (public/audio-capture-processor.js, public/pcm-processor.js)
**AudioWorklet Processors**: Low-latency audio processing in separate thread
- **audio-capture-processor.js**: Real-time audio capture and buffering
- **pcm-processor.js**: PCM audio processing and format conversion

**Key Features**:
- **Thread Isolation**: Audio processing in AudioWorklet thread for consistent performance
- **Buffer Management**: Efficient audio buffering with configurable chunk sizes
- **Format Conversion**: Real-time PCM conversion for Gemini Live Audio compatibility
- **Low Latency**: Optimized for real-time audio translation requirements

## Key Features Implemented

### 1. Real-time Audio Translation
- **Gemini Integration**: Uses Gemini 2.5 Flash Native Audio Dialog
- **Automatic Language Detection**: Based on participant configurations
- **Bi-directional Translation**: Supports all participant language pairs
- **Local Playback Control**: Toggle to control Gemini response playback
- **Audio Distribution**: Translated audio sent to all participants via WebSocket

### 2. Advanced Audio Processing
- **Dual Audio Contexts**: Separate 16kHz input and 24kHz output contexts
- **Rate Limiting**: Buffered audio sending (1.5 second intervals)
- **Speaking Detection**: Real-time audio level monitoring with threshold detection
- **Device Management**: Live microphone/speaker switching during conference
- **Enhanced Audio Processing**: AudioWorklet-based processing with thread isolation

### 3. WebRTC Communication
- **Peer-to-Peer**: Direct audio/video/screen share between participants
- **Signaling**: WebSocket-based signaling via Cloudflare Durable Objects
- **Track Management**: Dynamic audio track addition/removal based on settings
- **Screen Share**: Full screen sharing with remote display
- **Connection Management**: Robust connection handling with cleanup

### 4. Interactive Conference Features
- **Chat System**: Real-time messaging with read receipts and delivery status
- **Reactions**: Emoji reactions with timed display and animation
- **Hand Raise**: Speaking request system with visual indicators
- **Speaking Indicators**: Real-time visual feedback for active speakers
- **Participant Management**: Dynamic participant list with status updates

### 5. UI/UX Excellence
- **Responsive Design**: Mobile-first with desktop optimization
- **Accessibility**: Keyboard navigation, screen reader support
- **Error Handling**: Comprehensive error modals with user guidance
- **Settings Persistence**: LocalStorage for user preferences
- **Real-time Feedback**: Visual states for all interactive elements
- **WebGL Background**: Hardware-accelerated generative art with conference interactions

## Development Workflow

### Local Development with HTTPS (Required for Microphone Access)
Since WebRTC and `getUserMedia()` require HTTPS for microphone access, use Cloudflare Tunnels for local development:

```bash
# HTTPS local development with Cloudflare Tunnels
npm run dev:tunnel
```

This command:
1. Builds the frontend application
2. Starts local HTTP server on localhost:3000
3. Creates Cloudflare Tunnel with auto-generated HTTPS URL (e.g., `https://xxxxx.trycloudflare.com`)
4. Enables microphone access for audio translation features

**Alternative Local Development Scripts:**
```bash
# Local HTTP development (no microphone access)
npm run dev:frontend

# GitHub Pages deployment for HTTPS testing
npm run build && npm run deploy-gh-pages
```

### Standard Development Workflow
1. Make changes to modular files (main.tsx, components.tsx, hooks.ts, types.ts, generative-art-background-webgl.tsx)
2. Run `npm test` to ensure all unit tests pass (includes WebGL component tests)
3. Run `npm run test:api` to test deployed API (14 integration tests)
4. Run `npm run test:integration` to test Gemini Live Audio integration (4 tests)
5. Run `npm run test:all` to run comprehensive test suite (50+ total tests)
6. Run `npm run build` to bundle with esbuild (main.tsx entry point)
7. Commit and push to trigger GitHub Actions
8. Automated deployment and testing:
   - GitHub Pages deployment for frontend
   - Cloudflare Workers deployment for backend
   - **Automated API integration testing post-deployment**

## File Structure
```
/
├── public/
│   ├── index.html                   # Main HTML with Tailwind CDN
│   ├── bundle.js                    # Built React app
│   ├── pcm-processor.js             # AudioWorklet for PCM audio processing
│   ├── audio-capture-processor.js   # AudioWorklet for audio capture
│   ├── styles.css                   # Generated Tailwind CSS
│   ├── demo-*.html                  # WebGL demo files
│   └── favicon.svg                  # Monochrome project icon
├── Frontend (Modular):
│   ├── main.tsx                     # Application entry point
│   ├── components.tsx               # UI components and JSX structure
│   ├── hooks.ts                     # Custom hook with business logic
│   ├── types.ts                     # Interface and type definitions
│   ├── gemini-live-audio.ts         # Gemini Live Audio streaming module
│   ├── gemini-utils.ts              # Gemini audio processing utilities
│   ├── debug-utils.ts               # Debug utility functions
│   ├── translation-prompts.ts       # Multilingual system prompts
│   └── generative-art-background-webgl.tsx # WebGL generative art background
├── Backend (Modular):
│   ├── worker.js            # Main worker with routing
│   └── room-handler.js      # Durable Object for WebSocket room management
├── Legacy:
│   └── translation-conference-app.tsx  # Original monolithic component
├── Testing:
│   ├── tests/
│   │   ├── unit/                        # Unit tests with mocks
│   │   │   ├── translation-conference-app.test.tsx # Main app test suite (32 tests)
│   │   │   └── generative-art-background.test.tsx  # WebGL component tests (20 tests)
│   │   ├── integration/                 # Integration tests with real APIs
│   │   │   ├── api-integration.test.js  # API integration tests (14 tests)
│   │   │   └── gemini-live-audio.integration.test.ts # Gemini Live Audio tests (4 tests)
│   │   └── scripts/                     # Test utility scripts
│   ├── jest.config.js                   # Jest configuration for unit tests
│   ├── jest.setup.js                    # Jest setup for unit tests
│   ├── jest.api.config.js               # Jest configuration for API tests
│   ├── jest.api.setup.js                # Jest setup for API tests
│   ├── jest.integration.config.js       # Jest configuration for integration tests
│   └── jest.integration.setup.js        # Jest setup for integration tests
├── Mocks:
│   ├── __mocks__/@google/genai.ts       # Mock for Gemini API
│   └── __mocks__/gemini-live-audio.ts   # Mock for Live Audio module
├── Configuration:
│   ├── tsconfig.json        # TypeScript configuration
│   ├── package.json         # Dependencies and scripts (v0.8.0)
│   ├── wrangler.toml        # Cloudflare configuration
│   ├── tailwind.config.js   # Tailwind CSS configuration
│   └── global.d.ts          # Global TypeScript declarations
├── Documentation:
│   ├── README.md            # Project documentation
│   ├── .roorules            # Project rules and understanding
│   └── GEMINI_LIVE_AUDIO_INTEGRATION.md # Live Audio integration guide
├── Build Scripts:
│   └── scripts/build-with-commit.js     # Build script with commit hash injection
└── CI/CD:
    ├── .github/workflows/deploy-gh-pages.yml     # GitHub Pages deployment
    └── .github/workflows/deploy-cloudflare.yml   # Cloudflare Workers deployment
```

## Important Considerations
- **Modular Architecture**: Frontend and backend both use modular design for maintainability
- **Rich Feature Set**: Complete conference solution with chat, reactions, audio settings, speaking indicators
- **Real-time Translation**: Gemini Live Audio provides seamless multilingual communication
- **Advanced Audio Processing**: AudioWorklet-based processing with thread isolation and low latency
- **WebGL Acceleration**: GPU-accelerated particle system background with conference interactions
- **WebRTC Implementation**: Robust peer-to-peer communication with signaling
- **Error Handling**: Comprehensive error management throughout all systems
- **Performance**: Optimized for 60fps WebGL rendering and real-time audio processing
- **Accessibility**: Screen reader support and keyboard navigation
- **Testing**: Comprehensive test suite with unit, integration, and WebGL component coverage
- **Debug System**: Conditional logging for development and production
- **Language Support**: 25+ languages with cultural adaptations
- **UI/UX**: Responsive design with real-time feedback, animations, and dynamic backgrounds

## Testing Infrastructure
- **Unit Tests**: 52+ tests covering main application and WebGL components
  - Main application test suite (32 tests) with comprehensive WebRTC/Gemini mocking
  - WebGL component tests (20 tests) with canvas context and animation frame mocking
- **Integration Tests**: 18 tests covering real API interactions
  - API integration tests (14 tests) covering all Worker endpoints
  - Gemini Live Audio integration tests (4 tests) with real API calls
- **Test Categories**:
  - Component rendering and interaction testing
  - WebGL shader compilation and particle system testing
  - Audio processing and AudioWorklet testing
  - Health check endpoint (`/health`) testing
  - WebSocket endpoint (`/ws`) with room parameters testing
  - Error handling and performance testing
  - CORS and security headers validation
- **Automated Execution**: Tests run automatically after each deployment
- **Commands**: `npm run test`, `npm run test:api`, `npm run test:integration`, `npm run test:all`

## Cloudflare Deployment
- **Fixed Authentication Issues**: Resolved API Token permissions and account ID configuration
- **Improved Workflow**: Uses `cloudflare/wrangler-action@v3` for reliable deployment
- **Manual Trigger**: Added `workflow_dispatch` for manual deployment testing
- **Post-Deployment Testing**: Automatic API integration tests after successful deployment
- **Success Rate**: 100% deployment success after authentication fixes

## Gemini Live Audio Integration
- **Model**: Uses `models/gemini-2.5-flash-preview-native-audio-dialog`
- **Configuration**:
  - AUDIO modality only (TEXT+AUDIO causes INVALID_ARGUMENT error)
  - 16kHz PCM audio format for input, 24kHz for output
  - Automatic session management with proper cleanup
- **Audio Pipeline**:
  - Real-time audio capture from microphone
  - PCM conversion and base64 encoding
  - Chunked audio sending (~1.5 second intervals)
  - Automatic translated audio playback with device selection
- **Error Resolution**: Fixed INVALID_ARGUMENT errors by:
  - Using single modality configuration
  - Proper audio data format
  - System instruction-based translation context
- **Recent Enhancements**:
  - AudioWorklet-based PCM playback (following Google's official sample)
  - Automatic WAV header creation for raw PCM audio as fallback
  - Session state validation to prevent WebSocket errors
  - Memory leak prevention with proper cleanup
  - Enhanced error handling for audio processing
  - Local playback control for user preference
  - Token usage tracking and cost calculation
  - Dynamic language switching with session recreation</content>
## Performance Optimizations (v0.8.0)
- **WebGL Rendering**: GPU-accelerated particle system with 5000+ particles
- **AudioWorklet Processing**: Real-time audio processing in separate thread
- **Efficient Buffering**: Optimized audio chunk handling and memory management
- **Canvas Optimization**: Proper context reuse and buffer management
- **Particle System**: Advanced flow field generation with Perlin noise
- **Animation**: RequestAnimationFrame-based animation with 60fps target
- **Memory Management**: Proper cleanup and resource disposal